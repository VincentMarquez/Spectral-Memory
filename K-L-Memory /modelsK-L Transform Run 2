import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class KLMemory(nn.Module):
    """
    Spectral Covariance Memory (K-L Transform).
    
    Unlike the previous version which used a Time Kernel (sensitive to shuffling),
    this uses a Feature Covariance Kernel. It calculates the Eigenfunctions of the 
    latent space history, effectively performing PCA on the fly to extract 
    'Principal Temporal Patterns' (eigen-features) regardless of batch order.
    """
    def __init__(self, d_model, memory_depth=3000, n_components=32, memory_tokens=8):
        super().__init__()
        self.d_model = d_model
        self.memory_depth = memory_depth
        self.n_components = n_components
        self.memory_tokens = memory_tokens

        # Learnable projection to map the K-L components to Memory Tokens
        # Maps (n_components) -> (memory_tokens)
        self.component_mixer = nn.Linear(n_components, memory_tokens)
        self.norm = nn.LayerNorm(d_model)

        # We store raw hidden states: (T, d_model)
        self.register_buffer("_history", torch.zeros(0, d_model))

    def reset(self):
        self._history = torch.zeros(0, self.d_model, device=self._history.device)

    def append(self, h_states):
        """
        Appends new observations to the history buffer.
        h_states: (Batch, d_model)
        """
        # Detach to stop gradients flowing back into history indefinitely
        self._history = torch.cat([self._history, h_states.detach()], dim=0)
        
        # FIFO Eviction
        if self._history.shape[0] > self.memory_depth:
            self._history = self._history[-self.memory_depth:]

    def forward(self, B):
        """
        Computes the K-L decomposition of the history buffer to generate memory tokens.
        Returns: (memory_tokens, B, d_model)
        """
        T = self._history.shape[0]
        device = self._history.device
        
        # Cold start: return zeros if not enough history
        if T < self.n_components:
            return torch.zeros(self.memory_tokens, B, self.d_model, device=device)

        # --- 1. Compute Covariance Kernel (The "Slow" Deep Math) ---
        # We normalize the history to ensure stability
        H = F.normalize(self._history, p=2, dim=1) # (T, d_model)
        
        # Gram Matrix: Measures similarity between all stored historical states
        # (T, d_model) @ (d_model, T) -> (T, T)
        K = torch.mm(H, H.t()) 

        # Add jitter for numerical stability during Eigendecomposition
        K = K + 1e-6 * torch.eye(T, device=device)

        # --- 2. Eigen-Decomposition (Spectral Analysis) ---
        # This extracts the dominant "modes" of variation in the memory buffer
        # L: Eigenvalues, V: Eigenvectors
        # This is the computational bottleneck (O(T^3)) requested for "Deep Math"
        L, V = torch.linalg.eigh(K) 

        # --- 3. Select Top-k Principal Components ---
        # We take the last n_components (largest eigenvalues)
        idx = torch.argsort(L, descending=True)[:self.n_components]
        V_top = V[:, idx] # (T, n_components)
        
        # --- 4. Project History onto Eigenvectors ---
        # This gives us the "Eigen-Features" (Abstract representations of common patterns)
        # (d_model, T) @ (T, n_components) -> (d_model, n_components)
        principal_patterns = torch.mm(self._history.t(), V_top)
        
        # --- 5. Map Principal Components to Memory Tokens ---
        # We mix the n_components to form the requested number of memory tokens
        # (d_model, n_components) -> (d_model, memory_tokens)
        memory_basis = self.component_mixer(principal_patterns) 
        
        # Transpose to (memory_tokens, d_model)
        memory_basis = memory_basis.t()
        memory_basis = self.norm(memory_basis)

        # --- 6. Broadcast to Batch ---
        # The memory is "Global Context" shared across the batch
        # (Mem, d_model) -> (Mem, B, d_model)
        return memory_basis.unsqueeze(1).expand(-1, B, -1)


class Model(nn.Module):
    """
    KLMemory Transformer.
    
    Architecture Improvements:
    1. Channel Independence (CI): Treats every variable as a separate sequence.
    2. Flatten Head: Learns global trend/seasonality mapping instead of step-by-step decoding.
    3. Covariance Memory: Robust to data shuffling (unlike Time Kernels).
    """
    def __init__(self, configs):
        super().__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.c_out = configs.c_out
        self.enc_in = configs.enc_in
        self.d_model = configs.d_model
        
        # Channel Independence:
        # We treat each channel as a univariate series. 
        # Input dimension is effectively 1 per head.
        self.enc_embedding = nn.Linear(1, configs.d_model)
        
        # Positional Encoding
        self.pos_encoding = nn.Parameter(torch.randn(1, configs.seq_len, configs.d_model))
        nn.init.normal_(self.pos_encoding, std=0.02)

        # The Heavy K-L Memory
        self.memory = KLMemory(
            d_model=configs.d_model,
            memory_depth=2048,      # Deep history
            n_components=16,        # Number of Eigencomponents to extract
            memory_tokens=4         # Tokens injected into Transformer
        )

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=configs.d_model,
            nhead=configs.n_heads,
            dim_feedforward=configs.d_ff,
            dropout=configs.dropout,
            activation='gelu',
            batch_first=False,      # We use (Seq, Batch, Dim) for memory efficiency
            norm_first=True         # Pre-Norm is generally more stable
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=configs.e_layers)

        # SOTA Decoder: Flatten Head
        # Maps (Seq_Len * d_model) directly to (Pred_Len)
        self.head = nn.Linear(self.seq_len * configs.d_model, self.pred_len)
        self.dropout = nn.Dropout(configs.dropout)

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        # x_enc: [Batch, Seq_Len, Vars]
        
        # --- 1. RevIN / Normalization (Critical for SOTA) ---
        # We normalize per-instance to handle non-stationary data
        means = x_enc.mean(1, keepdim=True).detach()
        x_enc = x_enc - means
        stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
        x_enc /= stdev

        # --- 2. Channel Independence Reshape ---
        # [Batch, Seq, Vars] -> [Batch * Vars, Seq, 1]
        # This allows the model to learn specific frequencies for every channel independently
        B, L, C = x_enc.shape
        x_enc = x_enc.permute(0, 2, 1).reshape(B * C, L, 1)
        
        # --- 3. Embedding ---
        enc_out = self.enc_embedding(x_enc)         # [B*C, L, D]
        enc_out = enc_out + self.pos_encoding[:, :L, :]
        enc_out = self.dropout(enc_out)
        enc_out = enc_out.permute(1, 0, 2)          # [L, B*C, D] (Transformer standard)

        # --- 4. Memory Retrieval (Global Context) ---
        # Fetch memory for the expanded batch size (B*C)
        mem_tokens = self.memory(B * C)             # [Mem, B*C, D]
        
        # Prepend Memory to Sequence
        enc_input = torch.cat([mem_tokens, enc_out], dim=0) # [Mem+L, B*C, D]
        
        # --- 5. Transformer Encoder ---
        # Pass through Transformer
        enc_output = self.encoder(enc_input)        # [Mem+L, B*C, D]
        
        # Extract valid sequence (remove memory tokens)
        # The memory has injected global context into the sequence via Attention
        out = enc_output[self.memory.memory_tokens:] # [L, B*C, D]
        
        # --- 6. Persistent Memory Update ---
        # Update memory with the mean representation of the current batch
        # Only performed during training to build the "Latent Atlas"
        if self.training:
            # We detach to ensure we don't backprop through the entire history buffer
            # Average across time (L), keeping Batch*Channel separation
            batch_summary = out.mean(dim=0)         # [B*C, D]
            self.memory.append(batch_summary)

        # --- 7. Flatten Head Decoder ---
        # [L, B*C, D] -> [B*C, L*D] -> [B*C, Pred_Len]
        out = out.permute(1, 0, 2)
        out = out.reshape(B * C, -1)
        
        forecast = self.head(out)                   # [B*C, Pred_Len]
        
        # --- 8. Reshape back and Denormalize ---
        # [B*C, Pred_Len] -> [B, C, Pred_Len] -> [B, Pred_Len, C]
        forecast = forecast.reshape(B, C, -1).permute(0, 2, 1)
        
        # Denormalize
        forecast = forecast * stdev + means
        
        return forecast
