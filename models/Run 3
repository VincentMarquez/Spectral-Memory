import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class KLMemory(nn.Module):
    """
    Spectral Covariance Memory (K-L Transform) - Production Gold.
    Includes High-Res Spectral Analysis and Robust Normalization.
    """
    def __init__(self, d_model, memory_depth=3000, n_components=32, memory_tokens=8):
        super().__init__()
        self.d_model = d_model
        self.memory_depth = memory_depth
        self.n_components = n_components
        self.memory_tokens = memory_tokens

        # Maps eigen-components to memory tokens
        self.component_mixer = nn.Linear(n_components, memory_tokens)
        self.norm = nn.LayerNorm(d_model)

        self.register_buffer("_history", torch.zeros(0, d_model))

    def reset(self):
        self._history = torch.zeros(0, self.d_model, device=self._history.device)

    def append(self, h_states):
        # Detach to prevent infinite graph growth
        self._history = torch.cat([self._history, h_states.detach()], dim=0)
        if self._history.shape[0] > self.memory_depth:
            self._history = self._history[-self.memory_depth:]

    def forward(self, B):
        T = self._history.shape[0]
        device = self._history.device
        
        if T < self.n_components:
            return torch.zeros(self.memory_tokens, B, self.d_model, device=device)

        # 1. Covariance Kernel (Deep Math)
        H = F.normalize(self._history, p=2, dim=1)
        K = torch.mm(H, H.t()) 
        K = K + 1e-6 * torch.eye(T, device=device) # Jitter for stability

        # 2. Spectral Decomposition
        L, V = torch.linalg.eigh(K) 

        # 3. Top-k Eigenfeatures
        idx = torch.argsort(L, descending=True)[:self.n_components]
        V_top = V[:, idx] # (T, n_components)
        
        # 4. Projection (Latent Pattern Extraction)
        # (d_model, T) @ (T, n_components) -> (d_model, n_components)
        principal_patterns = torch.mm(self._history.t(), V_top)
        
        # 5. Token Generation
        memory_basis = self.component_mixer(principal_patterns) # (d_model, tokens)
        memory_basis = memory_basis.t() # (tokens, d_model)
        memory_basis = self.norm(memory_basis)

        return memory_basis.unsqueeze(1).expand(-1, B, -1)


class Model(nn.Module):
    """
    SOTA-Grade KLMemory Transformer (Gated).
    
    Key Features:
    1. Channel Independence (CI)
    2. Flatten Head Decoder
    3. Spectral Covariance Memory
    4. **Gated Injection**: Allows model to throttle memory usage.
    """
    def __init__(self, configs):
        super().__init__()
        self.seq_len = configs.seq_len
        self.pred_len = configs.pred_len
        self.c_out = configs.c_out
        self.enc_in = configs.enc_in
        self.d_model = configs.d_model
        
        # CI Embedding
        self.enc_embedding = nn.Linear(1, configs.d_model)
        self.pos_encoding = nn.Parameter(torch.randn(1, configs.seq_len, configs.d_model))
        nn.init.normal_(self.pos_encoding, std=0.02)

        # Memory
        self.memory = KLMemory(
            d_model=configs.d_model,
            memory_depth=3000,      
            n_components=32,        # Increased resolution
            memory_tokens=4         
        )
        
        # Gating Mechanism: Learnable parameter initialized to 0 (start neutral)
        self.memory_gate = nn.Parameter(torch.zeros(1))

        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=configs.d_model,
            nhead=configs.n_heads,
            dim_feedforward=configs.d_ff,
            dropout=configs.dropout,
            activation='gelu',
            batch_first=False,
            norm_first=True
        )
        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=configs.e_layers)

        # Decoder
        self.head = nn.Linear(self.seq_len * configs.d_model, self.pred_len)
        self.dropout = nn.Dropout(configs.dropout)

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):
        # 1. RevIN
        means = x_enc.mean(1, keepdim=True).detach()
        x_enc = x_enc - means
        stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
        x_enc /= stdev

        # 2. Channel Independence
        B, L, C = x_enc.shape
        x_enc = x_enc.permute(0, 2, 1).reshape(B * C, L, 1)
        
        # 3. Embed
        enc_out = self.enc_embedding(x_enc)
        enc_out = enc_out + self.pos_encoding[:, :L, :]
        enc_out = self.dropout(enc_out)
        enc_out = enc_out.permute(1, 0, 2) # [L, B*C, D]

        # 4. Memory Injection (Gated)
        mem_tokens = self.memory(B * C) # [Mem, B*C, D]
        
        # Apply Gate: tanh ensures range (-1, 1), allowing soft ignore or strong attention
        gate = torch.tanh(self.memory_gate) 
        mem_tokens = mem_tokens * gate
        
        enc_input = torch.cat([mem_tokens, enc_out], dim=0)

        # 5. Encoder
        enc_output = self.encoder(enc_input)
        out = enc_output[self.memory.memory_tokens:] # [L, B*C, D]
        
        # 6. Update Memory (Training Only)
        if self.training:
            batch_summary = out.mean(dim=0) # [B*C, D]
            self.memory.append(batch_summary)

        # 7. Decode
        out = out.permute(1, 0, 2).reshape(B * C, -1)
        forecast = self.head(out) # [B*C, Pred]
        
        # 8. Reshape & Denormalize
        forecast = forecast.reshape(B, C, -1).permute(0, 2, 1)
        forecast = forecast * stdev + means
        
        return forecast

