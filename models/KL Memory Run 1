# models/KLMemory.py

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, max_len: int = 10000):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(1))  # (max_len, 1, d_model)

    def forward(self, x: torch.Tensor, offset: int = 0):
        """
        x: (T, B, d_model)
        Returns x with positional encoding added, starting at the given offset.
        """
        T = x.size(0)
        return x + self.pe[offset:offset + T]


def _time_kernel(t, tau=64.0):
    """Exponential time kernel for K-L decomposition (radial basis in time)."""
    dt = (t[:, None] - t[None, :]).abs()
    return torch.exp(-dt / tau)

class KLMemory(nn.Module):
    def __init__(self, d_model, memory_depth=4096, n_components=16, tau=96.0, memory_tokens=8):
        super().__init__()
        self.d_model = d_model
        self.memory_depth = memory_depth
        self.n_components = n_components
        self.tau = tau
        self.memory_tokens = memory_tokens

        bottleneck = 64
        self.compress = nn.Linear(n_components * d_model, bottleneck, bias=False)
        self.expand = nn.Linear(bottleneck, memory_tokens * d_model, bias=False)
        self.dropout = nn.Dropout(0.1)

        self.register_buffer("_history", torch.zeros(0, d_model))
        self.register_buffer("_times", torch.zeros(0, dtype=torch.long))

    def reset(self):
        self._history = torch.zeros(0, self.d_model, device=self._history.device)
        self._times = torch.zeros(0, dtype=torch.long, device=self._times.device)

    def append(self, h_mean, t):
        self._history = torch.cat([self._history, h_mean.detach()], dim=0)
        self._times = torch.cat([self._times, t])
        if self._history.shape[0] > self.memory_depth:
            excess = self._history.shape[0] - self.memory_depth
            self._history = self._history[excess:]
            self._times = self._times[excess:]

    def forward(self, current_t, B):
        if self._history.shape[0] < self.n_components:
            return torch.zeros(self.memory_tokens, B, self.d_model, device=self._history.device)

        T = self._history.shape[0]
        t = self._times.float()

        # Fresh computation every time — safe and fast enough (T≤4096)
        dt = (t[:, None] - t[None, :]).abs()
        tau_scaled = self.tau / max(T, 1)
        K = torch.exp(-dt / tau_scaled)
        K = K + 1e-6 * torch.eye(T, device=K.device)
        K = 0.5 * (K + K.T)

        evals, evecs = torch.linalg.eigh(K)
        idx = torch.argsort(evals, descending=True)[:self.n_components]
        lams = torch.clamp(evals[idx], min=0)
        phi = evecs[:, idx]
        phi = phi / (phi.norm(dim=0, keepdim=True) + 1e-12)

        coeffs = phi.T @ self._history
        compressed = torch.sqrt(lams)[:, None] * coeffs

        flat = compressed.reshape(1, -1)
        h = F.gelu(self.compress(flat))
        h = self.dropout(h)
        tokens = self.expand(h).reshape(self.memory_tokens, self.d_model)

        return tokens.unsqueeze(1).expand(-1, B, -1)


class Model(nn.Module):

    def __init__(self, configs):
        super().__init__()
        self.task_name = configs.task_name
        self.seq_len = configs.seq_len
        self.label_len = configs.label_len
        self.pred_len = configs.pred_len
        self.d_model = configs.d_model
        self.c_out = configs.c_out

        self.in_proj = nn.Linear(configs.enc_in, configs.d_model)
        self.pos_enc = PositionalEncoding(configs.d_model)

        # Learnable positional embeddings for memory tokens (best practice)
        self.memory_pos_emb = nn.Parameter(torch.zeros(8, 1, configs.d_model))
        nn.init.normal_(self.memory_pos_emb, std=0.02)

        self.memory = KLMemory(
            d_model=configs.d_model,
            memory_depth=4096,      
            n_components=16,
            tau=96.0,               
            memory_tokens=8,
            
        )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=configs.d_model,
            nhead=configs.n_heads,
            dim_feedforward=configs.d_ff,
            dropout=configs.dropout,
            activation='gelu',
            batch_first=False
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=configs.e_layers)

        self.out_proj = nn.Linear(configs.d_model, configs.c_out)

    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        # Standard RevIN-style normalization used throughout TSL
        means = x_enc.mean(1, keepdim=True).detach()
        x_enc = x_enc - means
        stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)
        x_enc /= stdev

        B, L, _ = x_enc.shape

        # --- Encoder ---
        h = self.in_proj(x_enc).permute(1, 0, 2)               # (L, B, d)
        h = self.pos_enc(h, offset=self.memory.memory_tokens)   # positions start after memory tokens

        mem_tokens = self.memory(current_t=L, B=B)  # (M, B, d)

        h_full = torch.cat([mem_tokens, h], dim=0)             # (M+L, B, d)

        mask = nn.Transformer.generate_square_subsequent_mask(h_full.shape[0]).to(h_full.device)
        h_out = self.encoder(h_full, mask=mask)                 # (M+L, B, d)

        enc_out = h_out[self.memory.memory_tokens:]               # (L, B, d) — sequence representations

        # --- Simple decoder (repeat last label_len hidden states) ---
        repeat_times = math.ceil(self.pred_len / self.label_len)
        dec_out = enc_out[-self.label_len:].repeat(repeat_times, 1, 1)[:self.pred_len]

        y_pred = self.out_proj(dec_out)                         # (pred_len, B, c_out)
        y_pred = y_pred.permute(1, 0, 2)                      # (B, pred_len, c_out)

        # Denormalize
        y_pred = y_pred * stdev + means

        # --- Persistent memory update (one vector per sequence in batch) ---
        h_mean = enc_out.mean(dim=0)                           # (B, d_model) — summary per sequence
        current_t = self.memory._history.shape[0]
        t = torch.arange(current_t, current_t + B, device=x_enc.device, dtype=torch.long)
        self.memory.append(h_mean, t)

        return y_pred                                           # (B, pred_len, c_out) — correct TSL format

    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):
        if self.task_name in ['long_term_forecast', 'short_term_forecast']:
            return self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
        else:
            raise NotImplementedError("Only forecasting implemented")
